{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSLkT0qL3jgl"
      },
      "source": [
        "# **HW4P1: Language Modelling**\n",
        "\n",
        "Welcome to the final Part 1 HW of this course. This is the only part 1 in which you have PyTorch training (Yay!). You will be working on training language models and evaluating them on the task of prediction and generation.<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you go, please read the code and keep an eye out for TODOs.\n",
        "\n",
        "Structure of this notebook:\n",
        "\n",
        "- **Imports and installs** - specify the correct data paths and mostly just run it.\n",
        "- **Datasets** - complete TODO and run it.\n",
        "- **Dataloader** - complete TODO and run it.\n",
        "- **Language model architecture** - implement and define your preferred model architecture based on the writeup.\n",
        "- **Dataloader, model, loss, optimizer, and scheduler definition** - define your dataloader, model, loss, optimizer, and scheduler.\n",
        "- **Trainer class** - unlike all the P2s, we are using a Trainer class for this HW, review the class and complete the train function.\n",
        "- **Wandb** - add a correct API key.\n",
        "- **Experiments** - just run your experiments and note the resulting NLL metric.\n",
        "- **Evaluation** - get access to OpenAI API to get the resulting perplexity metric.\n",
        "- **Submission** - create a handin for Autolab."
      ],
      "metadata": {
        "id": "0TbDdinS6buu"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "95e48c7693e34a389da49dcb6e448e0c",
        "deepnote_cell_type": "markdown",
        "id": "EB2bOV3bzYLR"
      },
      "source": [
        "# **Imports and installs**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -xvf /content/hw4p1_handout.tar"
      ],
      "metadata": {
        "id": "tG9-HVopYWzX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r4_-qG9rSULt"
      },
      "outputs": [],
      "source": [
        "!pip install torchsummaryX\n",
        "!pip install wandb --quiet\n",
        "!pip install matplotlib\n",
        "\n",
        "!pip install -q cohere tiktoken openai\n",
        "!pip install openai==0.28"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RnrUvEIC5i5j"
      },
      "outputs": [],
      "source": [
        "# TODO: Import drive if you are using Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "03bf3bd639a048f098d5febc42e2baff",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 4,
        "execution_start": 1679856365820,
        "id": "QZNwme4320LW",
        "source_hash": "b7876178"
      },
      "outputs": [],
      "source": [
        "# You can upload entire working directory of HW4P1 to google drive and access the files from there\n",
        "\n",
        "import sys\n",
        "path = NotImplemented # TODO: Add path to handout. For example \"content/handout\"\n",
        "sys.path.append(path)\n",
        "%cd {path}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "b48a9e95f26c4d2e89d95b1b311cedd5",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution": {
          "iopub.execute_input": "2022-08-10T14:02:09.992480Z",
          "iopub.status.busy": "2022-08-10T14:02:09.987693Z",
          "iopub.status.idle": "2022-08-10T14:02:12.872562Z",
          "shell.execute_reply": "2022-08-10T14:02:12.870819Z",
          "shell.execute_reply.started": "2022-08-10T14:02:09.991351Z"
        },
        "execution_millis": 2669,
        "execution_start": 1679856365830,
        "id": "oxiZ42B4SwQ-",
        "source_hash": "ec149d26",
        "tags": []
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import torch\n",
        "\n",
        "import os\n",
        "\n",
        "import time\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from tqdm.notebook import tqdm\n",
        "import torchsummaryX\n",
        "import gc\n",
        "import wandb\n",
        "import yaml\n",
        "import openai\n",
        "\n",
        "# Importing necessary modules from hw4\n",
        "# Update the path depending on how you choose to load the handout\n",
        "from handout.hw4.tests_hw4 import get_prediction_nll, make_generation_text\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device: \", DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "a4ff875589ee46da8f749a7e5088a3ef",
        "deepnote_cell_type": "markdown",
        "id": "u-R794-0zc9V"
      },
      "source": [
        "# **Datasets**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HU4e_6l0Whda"
      },
      "outputs": [],
      "source": [
        "# Loading the vocabulary. Try printing and see\n",
        "VOCAB       = np.load('dataset/vocab.npy')\n",
        "\n",
        "# We have also included <sos> and <eos> in the vocabulary for you\n",
        "# However in real life, you include it explicitly if not provided\n",
        "SOS_TOKEN   = np.where(VOCAB == '<sos>')[0][0]\n",
        "EOS_TOKEN   = np.where(VOCAB == '<eos>')[0][0]\n",
        "NUM_WORDS   = len(VOCAB) - 2 # Actual number of words in vocabulary\n",
        "\n",
        "print(\"Vocab length: \", len(VOCAB))\n",
        "print(VOCAB)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LA7SapmyXHr7"
      },
      "outputs": [],
      "source": [
        "# Loding the training dataset. Refer to write up section 2 to understand the structure\n",
        "dataset     = np.load('dataset/wiki.train.npy', allow_pickle=True)\n",
        "\n",
        "# The dataset does not have <sos> and <eos> because they are just regular articles.\n",
        "# TODO: Add <sos> and <eos> to every article in the dataset.\n",
        "# Before doing so, try printing the dataset to see if they are words or integers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "09f3a2efaeef49ef9f4c0b2b9a614cca",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution": {
          "iopub.execute_input": "2022-08-10T14:02:12.888156Z",
          "iopub.status.busy": "2022-08-10T14:02:12.884281Z",
          "iopub.status.idle": "2022-08-10T14:02:12.960590Z",
          "shell.execute_reply": "2022-08-10T14:02:12.958805Z",
          "shell.execute_reply.started": "2022-08-10T14:02:12.888058Z"
        },
        "execution_millis": 46,
        "execution_start": 1679856368507,
        "id": "x5znxQhLSwRC",
        "source_hash": "42e4c03c",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Loading the fixtures for validation and test - prediction\n",
        "fixtures_pred       = np.load('fixtures/prediction.npz')        # validation\n",
        "fixtures_pred_test  = np.load('fixtures/prediction_test.npz')   # test\n",
        "\n",
        "print(\"Validation shapes    : \", fixtures_pred['inp'].shape, fixtures_pred['out'].shape)\n",
        "print(\"Test shapes          : \", fixtures_pred_test['inp'].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pes7mCr5WdAw"
      },
      "outputs": [],
      "source": [
        "# Loading the test fixtures for generation\n",
        "fixtures_gen_test   = np.load('fixtures/generation_test.npy')   # test\n",
        "\n",
        "print(\"Test Gen Shapes          :\", fixtures_gen_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jO_Qt7O6rL8L"
      },
      "outputs": [],
      "source": [
        "# Example Prediction Dev Input and Output\n",
        "# Optional TODO: You can try printing a few samples from the validation set which has both inputs and outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "aec0165a3f1245dfa52a0cb80dba2578",
        "deepnote_cell_type": "markdown",
        "id": "dHjYhXAOzkrP"
      },
      "source": [
        "# **Dataloader**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "b2e63a7f6dec4a3f98588725a72a8ff2",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution": {
          "iopub.execute_input": "2022-08-10T14:02:13.079390Z",
          "iopub.status.busy": "2022-08-10T14:02:13.078847Z",
          "iopub.status.idle": "2022-08-10T14:02:13.196189Z",
          "shell.execute_reply": "2022-08-10T14:02:13.192167Z",
          "shell.execute_reply.started": "2022-08-10T14:02:13.079324Z"
        },
        "execution_millis": 48,
        "execution_start": 1679856368575,
        "id": "OZNrJ8XvSwRF",
        "source_hash": "a81eaa14",
        "tags": []
      },
      "outputs": [],
      "source": [
        "class DataLoaderForLanguageModeling(torch.utils.data.DataLoader): # Inherit from torch.utils.data.DataLoader\n",
        "    \"\"\"\n",
        "        TODO: Define data loader logic here\n",
        "    \"\"\"\n",
        "    # TODO: You can probably add more parameters as well. Eg. sequence length\n",
        "    def __init__(self, dataset, batch_size, shuffle= True, drop_last= False):\n",
        "\n",
        "        # If you remember, these are the standard things which you give while defining a dataloader.\n",
        "        # Now you are just customizing your dataloader\n",
        "        self.dataset    = dataset\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle    = shuffle\n",
        "        self.drop_last  = drop_last\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        # What output do you get when you print len(loader)? You get the number of batches\n",
        "        # Your dataset has (579, ) articles and each article has a specified amount of words.\n",
        "        # You concatenate the dataset and then batch parts of it according to the sequence length\n",
        "\n",
        "        # TODO: return the number of batches\n",
        "        # If you are using variable sequence_length, the length might not be fixed\n",
        "\n",
        "        return NotImplemented\n",
        "\n",
        "    def __iter__(self):\n",
        "        # TODO: Shuffle data if shuffle is True\n",
        "        if self.shuffle:\n",
        "            # TODO\n",
        "            NotImplemented\n",
        "\n",
        "        # TODO: Set number of batches\n",
        "        num_batches = NotImplemented\n",
        "\n",
        "        # TODO: Concatenate articles and then drop extra words that won't fit into a full batch\n",
        "\n",
        "        # TODO: Think about how you could handle drop_last\n",
        "        if self.drop_last:\n",
        "            NotImplemented\n",
        "        else:\n",
        "            # Hint: Pad the last target sequence with EOS_TOKEN to ensure it has the same length as the other target sequences\n",
        "            NotImplemented\n",
        "\n",
        "        # TODO: Divide the concetenated dataset into inputs and targets. How do they vary?\n",
        "\n",
        "        # TODO: Reshape the inputs and targets into batches (think about the final shape)\n",
        "\n",
        "        # TODO: Loop though the batches and yield the input and target batch according to the sequence length\n",
        "        batch_idx = 0\n",
        "        while batch_idx < num_batches:\n",
        "            yield NotImplemented\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "773573c8374048d4bcb5a67b905ee2e0",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 3,
        "execution_start": 1679856368714,
        "id": "fBZSzmy10M9M",
        "source_hash": "27952b8c"
      },
      "outputs": [],
      "source": [
        "# Some sanity checks\n",
        "\n",
        "dl = DataLoaderForLanguageModeling(\n",
        "    dataset     = dataset,\n",
        "    batch_size  = 32,\n",
        "    shuffle     = True,\n",
        "    drop_last   = True,\n",
        "    # Input Extra parameters here if needed\n",
        ")\n",
        "\n",
        "inputs, targets = next(iter(dl))\n",
        "print(inputs.shape, targets.shape)\n",
        "\n",
        "for x, y in dl:\n",
        "    print(\"x: \", [VOCAB[i] for i in x[0, :]])\n",
        "    print(\"y: \", [VOCAB[i] for i in y[0, :]])\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "0e75c3c3318d481aa99230d81eb68c13",
        "deepnote_cell_type": "markdown",
        "id": "WcWU0YlnzmVM"
      },
      "source": [
        "# **Language model architecture**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recurrent network, one-directional or bidirectional, captures certain patterns within a sequence, and can store them into state vector or pass into output. As with convolutional networks, we can build another recurrent layer on top of the first one to capture higher level patterns, build from low-level patterns extracted by the first layer. This leads us to the notion of multi-layer RNN, which consists of two or more recurrent networks, where output of the previous layer is passed to the next layer as input.\n",
        "\n",
        "**Link to PyTorch Documentation**: [LSTM Cell](https://pytorch.org/docs/stable/generated/torch.nn.LSTMCell.html)\n",
        "\n",
        "The following image can be a helpful aid in visualizing the flow of information in a multi-layer RNN with LSTM Cells.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/microsoft/AI-For-Beginners/32043fd2c98de6bbcae857058ac38aaa8140b142/lessons/5-NLP/16-RNN/images/multi-layer-lstm.jpg\">\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_7wwkDXlV3xf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cebwoorWttWe"
      },
      "outputs": [],
      "source": [
        "# Here comes the main portion of this HW.\n",
        "# You can do this with a regular LSTM similar to HW3P2.\n",
        "# However, using LSTMCells presents an opportunity to learn something different\n",
        "\n",
        "class LanguageModel(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size): # TODO: Add more parameters as needed\n",
        "        super().__init__()\n",
        "\n",
        "        # For all the layers which you will define, please read the documentation thoroughly before implementation\n",
        "        # TODO: Define a PyTorch embedding layer\n",
        "        self.token_embedding    = NotImplemented\n",
        "\n",
        "        self.lstm_cells         = torch.nn.Sequential(\n",
        "            # TODO: Enter the parameters for the LSTMCells\n",
        "            torch.nn.LSTMCell(NotImplemented),\n",
        "            # You can add multiple LSTMCells and experiment with the shape of the network too if you want:\n",
        "        )\n",
        "\n",
        "        # (Optional) TODO: You can try weight initialization and see if they help\n",
        "\n",
        "        # TODO: Define the parameters\n",
        "        self.token_probability  = torch.nn.Linear(NotImplemented)\n",
        "\n",
        "        # (Optional) TODO: Weight Tying. You just need to make the embedding layer weights equal to the Linear layer weight.\n",
        "\n",
        "        # So the basic pipline is:\n",
        "        # word -> embedding -> lstm -> projection (linear) to get  probability distribution\n",
        "        # And this is happening across all time steps\n",
        "\n",
        "    def rnn_step(self, embedding, hidden_states_list):\n",
        "        next_hidden_states_list = []\n",
        "        token_embedding = embedding\n",
        "        # TODO: Forward pass through each LSTMCell\n",
        "\n",
        "        return token_embedding, next_hidden_states_list\n",
        "\n",
        "    def predict(self, x):\n",
        "        # Refer to Section 1.2.6 to understand this function\n",
        "        # if not torch.is_tensor(x):\n",
        "        x = torch.tensor(x).long().to(DEVICE)\n",
        "\n",
        "        with torch.inference_mode():\n",
        "            # TODO: Pass the input sequence through the model\n",
        "            # and return the probability distribution of the last timestep\n",
        "            return NotImplemented\n",
        "\n",
        "    def generate(self, x, timesteps):\n",
        "        # Refer to section 1.2.4 to understand this function\n",
        "        # Important Note: We do not draw <eos> from the distribution unlike the writeup\n",
        "\n",
        "        timesteps -= 1\n",
        "        # if not torch.is_tensor(x):\n",
        "        x = torch.tensor(x).long().to(DEVICE)\n",
        "\n",
        "        # TODO: Pass the input sequence through the model\n",
        "        # Obtain the probability distribution and hidden_states_list of the last timestep\n",
        "        token_prob_dist, hidden_states_list     = self.forward(x)\n",
        "\n",
        "        # TODO: Draw the next predicted token from the probability distribution ()\n",
        "        next_token                              = NotImplemented\n",
        "\n",
        "        # What would generated_sequence be initialized with?\n",
        "        generated_sequence  = [NotImplemented]\n",
        "        with torch.inference_mode():\n",
        "            for t in range(timesteps): # Loop through the timesteps\n",
        "\n",
        "                # TODO: Pass the next_token and hidden_states_list through the model\n",
        "\n",
        "                # TODO: You will get 2 outputs. What is the shape of the probability distribution?\n",
        "\n",
        "                # TODO: Get the most probable token for the next timestep\n",
        "\n",
        "                generated_sequence.append(next_token)\n",
        "\n",
        "            generated_sequence = torch.stack(generated_sequence, dim= 1) # keep last timesteps generated words\n",
        "        return generated_sequence.squeeze(-1)\n",
        "\n",
        "    # We are also having a hidden_states_list parameter because you need that in generation\n",
        "    def forward(self, x, hidden_states_list= None): # train model\n",
        "        # x (Batch, Seq_len)\n",
        "        # Note: you dont have to return the sum of log probabilities according to Pseudocode 1 in the writeup\n",
        "        # However, feel free to calculate and print it if you are curious\n",
        "        x = x.long()\n",
        "\n",
        "        batch_size, timesteps   = x.shape\n",
        "\n",
        "        token_prob_distribution = [] # list which will contain probability distributions for all timesteps\n",
        "\n",
        "        # Initializing the hidden hidden_states_list\n",
        "        # Are the elements of the hidden_states_list individual variables or lists of variables themselves?\n",
        "        # Hint: Refer the PyTorch documentation for the answer\n",
        "        hidden_states_list      = [None]*len(self.lstm_cells) if hidden_states_list == None else hidden_states_list\n",
        "\n",
        "        token_embeddings        = NotImplemented # TODO\n",
        "\n",
        "        # When you get the embeddings of the input x, remember that you get it for all time steps.\n",
        "        # Embedding is just a linear transformation so you can precompute it for all time steps.\n",
        "\n",
        "        for t in range(timesteps): # LSTMCell is for just 1 timestep. Hence you need to loop through the total timesteps\n",
        "\n",
        "            token_embedding_t   = NotImplemented # TODO\n",
        "\n",
        "            # TODO (What should you do with the hidden_states_list?)\n",
        "            rnn_out, hidden_states_list = NotImplemented\n",
        "\n",
        "            # Map the RNN output to the vocabulary’s dimension and store it in token_prob_dist_t,\n",
        "            # the token probability distribution at time t.\n",
        "            token_prob_dist_t   = NotImplemented # TODO\n",
        "\n",
        "            # Append token_prob_dist_t to a list of token probability distributions.\n",
        "            token_prob_distribution.append(token_prob_dist_t)\n",
        "\n",
        "        # TODO: Stack along the timesteps dimension\n",
        "        token_prob_distribution = NotImplemented\n",
        "\n",
        "        return token_prob_distribution, hidden_states_list"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Dataloader, model, loss, optimizer, and scheduler definition**"
      ],
      "metadata": {
        "id": "DelhoytAQWQa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Define other hyperparameters\n",
        "\n",
        "config = dict(\n",
        "    batch_size  = 256,\n",
        "    num_epochs  = 10, # 10 to 20 epochs should be enough given the model is good\n",
        "    init_lr     = NotImplemented # TODO\n",
        ")"
      ],
      "metadata": {
        "id": "jW-YAD9b7FoC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "4aaccf1c32fa480a9a15e8bb8bc4d9e4",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": true,
        "execution": {
          "iopub.execute_input": "2022-08-10T14:02:14.110787Z",
          "iopub.status.busy": "2022-08-10T14:02:14.109778Z",
          "iopub.status.idle": "2022-08-10T14:02:14.929087Z",
          "shell.execute_reply": "2022-08-10T14:02:14.925078Z",
          "shell.execute_reply.started": "2022-08-10T14:02:14.110707Z"
        },
        "id": "DbHH6zXTSwRa",
        "source_hash": "2acff566",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# --------------------------------------------------- #\n",
        "\n",
        "# TODO: Define the dataloader\n",
        "loader = DataLoaderForLanguageModeling(NotImplemented)\n",
        "\n",
        "# --------------------------------------------------- #\n",
        "\n",
        "# TODO: Define the model\n",
        "model = NotImplemented\n",
        "\n",
        "# --------------------------------------------------- #\n",
        "\n",
        "# TODO: Define the criterion\n",
        "criterion   = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# --------------------------------------------------- #\n",
        "\n",
        "# TODO: Define the optimizer\n",
        "## Adam/AdamW usually works good for this HW\n",
        "optimizer   = NotImplemented\n",
        "\n",
        "# --------------------------------------------------- #\n",
        "\n",
        "# TODO: Define scheduler\n",
        "scheduler = NotImplemented\n",
        "\n",
        "# --------------------------------------------------- #\n",
        "\n",
        "#TODO: Define scaler for mixed precision\n",
        "scaler = NotImplemented\n",
        "\n",
        "print(model)\n",
        "summary = torchsummaryX.summary(model.to(DEVICE), x = torch.tensor(inputs).to(DEVICE))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "8ed5a6ef54f9446fab752b79c70a0216",
        "deepnote_cell_type": "markdown",
        "id": "TlWF_bpLznup"
      },
      "source": [
        "# **Trainer class**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "8ea986fc372643389d1ab4c445659e9d",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": true,
        "execution": {
          "iopub.execute_input": "2022-08-10T14:02:13.440820Z",
          "iopub.status.busy": "2022-08-10T14:02:13.440281Z",
          "iopub.status.idle": "2022-08-10T14:02:13.644455Z",
          "shell.execute_reply": "2022-08-10T14:02:13.642614Z",
          "shell.execute_reply.started": "2022-08-10T14:02:13.440752Z"
        },
        "id": "kIvZOIfjSwRK",
        "source_hash": "451a140f",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Unlike all the P2s, we are using a Trainer class for this HW.\n",
        "# Many researchers also use classes like this for training. You may have encountered them in your project as well.\n",
        "# You dont have to complete everything in this class, you only need to complete the train function.\n",
        "# However, its good to go through the code and see what it does.\n",
        "\n",
        "class Trainer:\n",
        "    def __init__(self, model, loader, optimizer, criterion, scheduler, scaler, max_epochs= 1, run_id= 'exp'):\n",
        "        \"\"\"\n",
        "            Use this class to train your model\n",
        "        \"\"\"\n",
        "        # feel free to add any other parameters here\n",
        "        self.model      = model\n",
        "        self.loader     = loader\n",
        "        self.optimizer  = optimizer\n",
        "        self.criterion  = criterion\n",
        "        self.scheduler  = scheduler\n",
        "        self.scaler     = scaler\n",
        "\n",
        "        self.train_losses           = []\n",
        "        self.val_losses             = []\n",
        "        self.prediction_probs       = []\n",
        "        self.prediction_probs_test  = []\n",
        "        self.generated_texts_test   = []\n",
        "        self.epochs                 = 0\n",
        "        self.max_epochs             = max_epochs\n",
        "        self.run_id                 = run_id\n",
        "\n",
        "\n",
        "    def calculate_loss(self, out, target):\n",
        "        # output: (B, T, Vocab_size) - probability distributions\n",
        "        # target: (B, T)\n",
        "        # Read the documentation of CrossEntropyLoss and try to understand how it takes inputs\n",
        "\n",
        "        # Tip: If your target is of shape (B, T) it means that you have B batches with T words.\n",
        "        # Tip: What is the total number of words in this batch?\n",
        "        # Tip: Crossentropy calculates the loss between a label and its probability distribution.\n",
        "\n",
        "        out     = NotImplemented # TODO\n",
        "        targets = NotImplemented # TODO: Reshape as necessary\n",
        "        loss    = self.criterion(out, targets)\n",
        "\n",
        "        return loss\n",
        "\n",
        "\n",
        "    def train(self):\n",
        "\n",
        "        self.model.train() # set to training mode\n",
        "        self.model.to(DEVICE)\n",
        "        epoch_loss  = 0\n",
        "        num_batches = 0\n",
        "\n",
        "        for batch_num, (inputs, targets) in enumerate(tqdm(self.loader)):\n",
        "\n",
        "            # TODO: Complete the loop. You should be able to complete this without any helper comments after 3 HWs\n",
        "            # Tip: Use Mixed Precision Training\n",
        "            # For loss calculation, use the calculate_loss function. You need to complete it before using.\n",
        "\n",
        "            inputs = torch.tensor(inputs).long().to(DEVICE)\n",
        "            targets = torch.tensor(targets).long().to(DEVICE)\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "\n",
        "            with torch.cuda.amp.autocast():\n",
        "              # Add code here\n",
        "              pass\n",
        "            loss_item = loss.item()\n",
        "            epoch_loss += loss_item\n",
        "\n",
        "            # TODO: Add backward and, optimiser step and scaler update code here:\n",
        "\n",
        "        epoch_loss = epoch_loss / (batch_num + 1)\n",
        "        self.epochs += 1\n",
        "        print('[TRAIN] \\tEpoch [%d/%d] \\tLoss: %.4f \\tLr: %.6f'\n",
        "                    % (self.epochs, self.max_epochs, epoch_loss, self.optimizer.param_groups[0]['lr']))\n",
        "        self.train_losses.append(epoch_loss)\n",
        "\n",
        "        return (epoch_loss, self.optimizer.param_groups[0]['lr'])\n",
        "\n",
        "\n",
        "\n",
        "    def test(self): # Don't change this function\n",
        "\n",
        "        self.model.eval() # set to eval mode\n",
        "        prediction_probs     = self.model.predict(fixtures_pred['inp']).detach().cpu().numpy() # get predictions\n",
        "        self.prediction_probs.append(prediction_probs)\n",
        "\n",
        "        generated_indexes_test   = self.model.generate(fixtures_gen_test, 10).detach().cpu().numpy() # generated predictions for 10 words\n",
        "\n",
        "        nll                   = get_prediction_nll(prediction_probs, fixtures_pred['out'])\n",
        "        generated_texts_test  = make_generation_text(fixtures_gen_test, generated_indexes_test, VOCAB)\n",
        "        self.val_losses.append(nll)\n",
        "\n",
        "        self.generated_texts_test.append(generated_texts_test)\n",
        "\n",
        "        # generate predictions for test data\n",
        "        prediction_probs_test = self.model.predict(fixtures_pred_test['inp']).detach().cpu().numpy() # get predictions\n",
        "        self.prediction_probs_test.append(prediction_probs_test)\n",
        "\n",
        "        print('[VAL] \\tEpoch [%d/%d] \\tLoss: %.4f'\n",
        "                      % (self.epochs, self.max_epochs, nll))\n",
        "        return nll\n",
        "\n",
        "\n",
        "    def save(self): # Don't change this function\n",
        "\n",
        "        model_path = os.path.join('hw4/experiments', self.run_id, 'model-{}.pkl'.format(self.epochs))\n",
        "        torch.save({'state_dict': self.model.state_dict()}, model_path)\n",
        "        np.save(os.path.join('hw4/experiments', self.run_id, 'prediction-probs-{}.npy'.format(self.epochs)), self.prediction_probs[-1])\n",
        "        np.save(os.path.join('hw4/experiments', self.run_id, 'prediction-probs-test-{}.npy'.format(self.epochs)), self.prediction_probs_test[-1])\n",
        "\n",
        "        with open(os.path.join('hw4/experiments', self.run_id, 'generated-texts-{}-test.txt'.format(self.epochs)), 'w') as fw:\n",
        "            fw.write(self.generated_texts_test[-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "aaff53cf948e44b7b9bd49cbcad0ac58",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": true,
        "execution": {
          "iopub.execute_input": "2022-08-10T14:02:13.931258Z",
          "iopub.status.busy": "2022-08-10T14:02:13.930204Z",
          "iopub.status.idle": "2022-08-10T14:02:14.107883Z",
          "shell.execute_reply": "2022-08-10T14:02:14.105987Z",
          "shell.execute_reply.started": "2022-08-10T14:02:13.931185Z"
        },
        "id": "2HCVG5YISwRW",
        "source_hash": "c9f4594a",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Dont change this cell\n",
        "\n",
        "run_id = str(int(time.time()))\n",
        "if not os.path.exists('./hw4/experiments'):\n",
        "    os.mkdir('./hw4/experiments')\n",
        "os.mkdir('./hw4/experiments/%s' % run_id)\n",
        "print(\"Saving models, prediction prbabilities, and generated texts to ./hw4/experiments/%s\" % run_id)\n",
        "\n",
        "# The object of the Trainer class takes in everything\n",
        "trainer = Trainer(\n",
        "    model       = model,\n",
        "    loader      = loader,\n",
        "\n",
        "    optimizer   = optimizer,\n",
        "    criterion   = criterion,\n",
        "    scheduler   = scheduler,\n",
        "    scaler      = scaler,\n",
        "    max_epochs  = config['num_epochs'],\n",
        "    run_id      = run_id\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Wandb**"
      ],
      "metadata": {
        "id": "Dfrf1FoSoAI0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use wandb? Resume Training?\n",
        "USE_WANDB = True\n",
        "RESUME_LOGGING = False\n",
        "\n",
        "# Create your wandb run\n",
        "\n",
        "run_name = NotImplemented\n",
        "\n",
        "if USE_WANDB:\n",
        "\n",
        "    wandb.login(key=NotImplemented)\n",
        "\n",
        "    if RESUME_LOGGING:\n",
        "        run_id = ''\n",
        "        run = wandb.init(\n",
        "            id     = run_id, ### Insert specific run id here if you want to resume a previous run\n",
        "            resume = \"must\", ### You need this to resume previous runs, but comment out reinit = True when using this\n",
        "            project = \"hw4p1-s24\", ### Project should be created in your wandb account\n",
        "        )\n",
        "    else:\n",
        "        run = wandb.init(\n",
        "            name    = run_name, ### Wandb creates random run names if you skip this field, we recommend you give useful names\n",
        "            reinit  = True, ### Allows reinitalizing runs when you re-run this cell\n",
        "            project = \"hw4p1-s24\", ### Project should be created in your wandb account\n",
        "            config  = config ### Wandb Config for your run\n",
        "        )\n",
        "\n",
        "        ### Save your model architecture as a string with str(model)\n",
        "        model_arch  = str(model)\n",
        "        ### Save it in a txt file\n",
        "        arch_file   = open(\"model_arch.txt\", \"w\")\n",
        "        file_write  = arch_file.write(model_arch)\n",
        "        arch_file.close()\n",
        "\n",
        "        ### log it in your wandb run with wandb.save()\n",
        "        wandb.save('model_arch.txt')"
      ],
      "metadata": {
        "id": "EuIbwKXToCt-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Experiments**"
      ],
      "metadata": {
        "id": "0fcxKXL0hrxX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the experiments loop.\n",
        "# Each epoch wont take more than 2-3min. If its taking more time, it might be due to (but not limited to) the following:\n",
        "#   * You might be overlapping batches\n",
        "#       Eg. Input: \"I had biryani for lunch today\" and sequence length = 3,\n",
        "#           --> \"I had biryani\", \"for lunch today\" are ideal examples for inputs\n",
        "#           --> \"I had biryani\", \"had biryani for\", \"biryani for lunch\", ... is just redundant info :')\n",
        "#   * Your length calculation in the dataloader might be wrong\n",
        "# If you haven't had biryani, try it :D\n",
        "\n",
        "wandb.watch(model, log=\"all\")\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "# %%time\n",
        "best_nll = 1e30\n",
        "for epoch in range(config['num_epochs']):\n",
        "    train_loss, curr_lr = NotImplemented\n",
        "    nll = trainer.test()\n",
        "    if nll < best_nll:\n",
        "        best_nll = nll\n",
        "        print(\"Saving model, prediction probabilities and generated texts for epoch \"+str(epoch+1)+\" with NLL: \"+ str(best_nll))\n",
        "        trainer.save()\n",
        "\n",
        "    wandb.log({\"train_loss\":train_loss,\n",
        "               \"nll\": nll,\n",
        "               \"learning_rate\": curr_lr\n",
        "              })\n",
        "\n",
        "### Finish your wandb run\n",
        "run.finish()"
      ],
      "metadata": {
        "id": "-pLh5_LCpVxw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Oq8tLAo7jF_"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i_gYqXq9Jgo1"
      },
      "outputs": [],
      "source": [
        "plt.figure()\n",
        "plt.plot(range(1, trainer.epochs + 1), trainer.train_losses, label='Training losses')\n",
        "plt.plot(range(1, trainer.epochs + 1), trainer.val_losses, label='Validation losses')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('NLL')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vleoDhQe7jF_"
      },
      "source": [
        "# **Evaluation**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " - Now that you have trained your model and got satisfactory validation NLL on the single token prediction task, you can evaluate the generations you created too\n",
        " - We will use the perplexity metric to evaluate generations using a large language model available through the OpenAI API. Read the handout for instructions on how to sign up for the API and obtain and API key.\n",
        " - Once you add credits to your account, run this cell to get the perplexity.\n",
        " - You will submit this perplexity value for grading the generation component of this homework.\n",
        " - A perplexity of under **1400** will give you full credit on the generation part."
      ],
      "metadata": {
        "id": "BbDQm49BKrdd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Change only the **submission_run_id**, **submission_epoch**, and **api_key** in the following cell"
      ],
      "metadata": {
        "id": "PvVGQNzUL3IK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qqJ9lf7W7jF_"
      },
      "outputs": [],
      "source": [
        "# DO NOT CHANGE THE CODE IN THIS CELL EXCEPT submission_run_id, submission_epoch, AND api_key\n",
        "# PLEASE BE HONEST IN REPORTING THE PERPLEXITY VALUE!\n",
        "# WE WILL RANDOMLY CHECK SOME SUBMISSIONS USING THE SAME CODE AS THIS AND A BIG DIFFERENCE IN PERPLEXITY WILL RESULT IN AN AIV.\n",
        "\n",
        "import openai\n",
        "\n",
        "# Add you submission_run_id and submission_epoch here --------------------------------------------------\n",
        "# Fill the run id and epoch number to be used for submission.\n",
        "# You will use the same run id and epoch number to generate the handin.\n",
        "\n",
        "submission_run_id = NotImplemented # TODO\n",
        "submission_epoch = NotImplemented # TODO\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "\n",
        "n_tests = 128\n",
        "\n",
        "with open(os.path.join('hw4/experiments', submission_run_id, 'generated-texts-{}-test.txt'.format(submission_epoch)), 'r', encoding='utf-8') as f:\n",
        "    generated = list(f)\n",
        "\n",
        "assert len(generated) == n_tests\n",
        "for item in generated:\n",
        "    assert type(item) is str\n",
        "\n",
        "parsed_generated = []\n",
        "\n",
        "for text in generated:\n",
        "    start_index = text.index(\"<sos>\")\n",
        "    temp = text[start_index+6:]\n",
        "    generation_start_index = temp.index(\"| \")\n",
        "    parsed_text = temp[:generation_start_index] + temp[generation_start_index+2:]\n",
        "    parsed_text = parsed_text.replace(\"<eol>\", \"\\n\")\n",
        "    parsed_generated.append(parsed_text)\n",
        "\n",
        "def perplexity(text, modelname):\n",
        "    \"\"\"Compute the perplexity of the provided text.\"\"\"\n",
        "    completion = openai.Completion.create(\n",
        "        model=modelname,\n",
        "        prompt=text,\n",
        "        logprobs=0,\n",
        "        max_tokens=0,\n",
        "        temperature=1.0,\n",
        "        echo=True)\n",
        "    token_logprobs = completion['choices'][0]['logprobs']['token_logprobs']\n",
        "    ll = np.mean([i for i in token_logprobs if i is not None])\n",
        "    ppl = np.exp(-ll)\n",
        "    return ppl\n",
        "\n",
        "# Add you API key here --------------------------------------------------\n",
        "# However, delete the key from the notebook before creating the handin.\n",
        "# REMEMBER: ALWAYS KEEP YOUR API KEYS AND SECRETS SECURE.\n",
        "\n",
        "openai.api_key = 'Enter_API_Key_Here' # TODO\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "\n",
        "modelname = 'text-embedding-ada-002'\n",
        "\n",
        "perps = [perplexity(text, modelname) for text in tqdm(parsed_generated)]\n",
        "avg_perp = np.mean(perps)\n",
        "\n",
        "# Report this number when running the makefile to create the handin\n",
        "print(\"Your mean perplexity for generated sequences: {}\".format(avg_perp))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOTE**:\n",
        "- If you get a \"The server is overloaded or not ready yet\" when trying to run the above cell, simply try re-running after some time.\n",
        "- You will need to add credits ($5) to your open-ai account to get rid of the limit error."
      ],
      "metadata": {
        "id": "-g3iGbFGXjC3"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBqjqy-EyU27"
      },
      "source": [
        "# **Submission**\n",
        "Navigate to the handout directory to run the below cell. This command will create the handin with all the required files (including attention.py). So make sure you have the entire handout directory wherever you are running this notebook (local machine, Colab, AWS, etc.).\n",
        "\n",
        "**IMPORTANT NOTE:** This command requires that this c**ompleted notebook be in the hw4 folder inside the handout directory**. If you are on colab, this notebook you are working on **DOES NOT** live in the handout directory. You must **download it and then upload it** to the hw4 folder replacing the empty starter notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hWRyPvWmgLQs"
      },
      "outputs": [],
      "source": [
        "# TODO: Generate the handin to submit to autolab\n",
        "\n",
        "# For example:\n",
        "# !make runid=1705009752 epoch=9 ppl=1287.0752467922216\n",
        "\n",
        "!make runid=<Enter_run_id_here> epoch=<Enter_epoch_number_here> ppl=<Enter_perplexity_value_here>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "THcllS3fnFR3"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "collapsed_sections": [
        "EB2bOV3bzYLR",
        "WcWU0YlnzmVM",
        "DelhoytAQWQa",
        "TlWF_bpLznup",
        "Dfrf1FoSoAI0",
        "0fcxKXL0hrxX",
        "vleoDhQe7jF_",
        "BBqjqy-EyU27"
      ],
      "toc_visible": true
    },
    "deepnote": {},
    "deepnote_execution_queue": [],
    "deepnote_notebook_id": "989a3c3836794109ac641230122845a3",
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}